{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91df75f8-ce74-4778-abe9-a6eebd66730c",
   "metadata": {},
   "source": [
    "# A detailed how-to of Seismic.\n",
    "\n",
    "## This Jupyter notebook is a more detailed documentation of how to use Seismic and all its functionalities.\n",
    "\n",
    "## For questions, feel free to open a GitHub issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "97bb4212",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyseismic-lsr in ./.venv/lib/python3.10/site-packages (0.4.3)\n",
      "Requirement already satisfied: numpy in ./.venv/lib/python3.10/site-packages (from pyseismic-lsr) (2.2.6)\n"
     ]
    }
   ],
   "source": [
    "# ONLY ONCE\n",
    "!python -m pip install pyseismic-lsr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6a5a041",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seismic import SeismicIndex, SeismicDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c912a968",
   "metadata": {},
   "source": [
    "# 1. Indexing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dae18f-6083-4ed7-b950-cdfab69edc29",
   "metadata": {},
   "source": [
    "## 1.0 Building from in-memory vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0769a052-bf2f-4206-98d3-b69c39ae6844",
   "metadata": {},
   "source": [
    "### We can build an index from numpy vectors loaded in memory. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b1ed3c56-48e7-4f57-90cb-1b61982e6aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "\n",
    "import seismic\n",
    "from seismic import SeismicDataset # -> SeismicDataset allows to incrementally add vectors and then build the SeismicIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc88140b-12b1-4057-8c80-219144ec7ea2",
   "metadata": {},
   "source": [
    "### Load the vectors into the dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee9a4a91",
   "metadata": {},
   "source": [
    "## 1.1 Building from `jsonl` or `tar.gz`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3696691",
   "metadata": {},
   "source": [
    "### We can build the index either from a jsonl file or a compressed archive tar.gz containing the jsonl file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eefe4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_input_file = \"mMARCO/Dataset_Json/English/mMARCO_english_vectors_multi_unicoil_xlmr_topk.jsonl\"\n",
    "# compressed_input_file = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feb5215e",
   "metadata": {},
   "source": [
    "### We can use the default configuration by specifying only the input file or choose each of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2b8be34",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'SeismicModel' from 'seismic' (/home/eli/Multilingual_IE_Seismic/.venv/lib/python3.10/site-packages/seismic/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# bruach ich nimmer?\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseismic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SeismicModel\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mjson\u001b[39;00m\n\u001b[1;32m      6\u001b[0m model \u001b[38;5;241m=\u001b[39m SeismicModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseismic-base\u001b[39m\u001b[38;5;124m\"\u001b[39m)   \u001b[38;5;66;03m# or multilingual model later\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'SeismicModel' from 'seismic' (/home/eli/Multilingual_IE_Seismic/.venv/lib/python3.10/site-packages/seismic/__init__.py)"
     ]
    }
   ],
   "source": [
    "# bruach ich nimmer?\n",
    "\n",
    "from seismic import SeismicModel\n",
    "import json\n",
    "\n",
    "model = SeismicModel.from_pretrained(\"seismic-base\")   # or multilingual model later\n",
    "\n",
    "input_path = \"mMARCO/Dataset_Json/English/mMARCO_collection_converted-english.jsonl\"         # original file\n",
    "output_path = \"mMARCO/Dataset_Json/English/mMARCO_collection_converted-english_vectors.jsonl\"\n",
    "\n",
    "with open(input_path, \"r\") as fin, open(output_path, \"w\") as fout:\n",
    "    for i, line in enumerate(fin):\n",
    "        if i >= 10:\n",
    "            break  # stop after 10 documents\n",
    "        doc = json.loads(line)\n",
    "        text = doc[\"contents\"]\n",
    "\n",
    "        # compute sparse vector\n",
    "        sparse_vec = model.encode({\"contents\": text})  # returns dict of term: weight\n",
    "\n",
    "        # add vector field\n",
    "        doc[\"vector\"] = sparse_vec\n",
    "\n",
    "        # write\n",
    "        fout.write(json.dumps(doc) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4b15fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = SeismicIndex.build(json_input_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "08e55de8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 00:12:27] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 00:12:27] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 00:12:27] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 00:12:29] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 00:12:29] CPU Model on constant consumption mode: AMD Ryzen 9 5900X 12-Core Processor\n",
      "[codecarbon WARNING @ 00:12:29] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 00:12:29] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 00:12:29] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 00:12:29] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: global constant\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 00:12:29] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 00:12:29]   Platform system: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 00:12:29]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 00:12:29]   CodeCarbon version: 3.1.1\n",
      "[codecarbon INFO @ 00:12:29]   Available RAM : 15.578 GB\n",
      "[codecarbon INFO @ 00:12:29]   CPU count: 24 thread(s) in 1 physical CPU(s)\n",
      "[codecarbon INFO @ 00:12:29]   CPU model: AMD Ryzen 9 5900X 12-Core Processor\n",
      "[codecarbon INFO @ 00:12:29]   GPU count: 1\n",
      "[codecarbon INFO @ 00:12:29]   GPU model: 1 x NVIDIA GeForce RTX 2060\n",
      "[codecarbon INFO @ 00:12:32] Emissions data (if any) will be saved to file /home/eli/Multilingual_IE_Seismic/emissions.csv\n",
      "[codecarbon WARNING @ 00:12:32] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 00:12:32] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 00:12:32] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 00:12:33] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Linux OS detected: Please ensure RAPL files exist, and are readable, at /sys/class/powercap/intel-rapl/subsystem to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 00:12:33] CPU Model on constant consumption mode: AMD Ryzen 9 5900X 12-Core Processor\n",
      "[codecarbon WARNING @ 00:12:33] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 00:12:33] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 00:12:33] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 00:12:33] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: global constant\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 00:12:33] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 00:12:33]   Platform system: Linux-6.6.87.2-microsoft-standard-WSL2-x86_64-with-glibc2.35\n",
      "[codecarbon INFO @ 00:12:33]   Python version: 3.10.12\n",
      "[codecarbon INFO @ 00:12:33]   CodeCarbon version: 3.1.1\n",
      "[codecarbon INFO @ 00:12:33]   Available RAM : 15.578 GB\n",
      "[codecarbon INFO @ 00:12:33]   CPU count: 24 thread(s) in 1 physical CPU(s)\n",
      "[codecarbon INFO @ 00:12:33]   CPU model: AMD Ryzen 9 5900X 12-Core Processor\n",
      "[codecarbon INFO @ 00:12:33]   GPU count: 1\n",
      "[codecarbon INFO @ 00:12:33]   GPU model: 1 x NVIDIA GeForce RTX 2060\n",
      "[codecarbon INFO @ 00:12:36] Emissions data (if any) will be saved to file /home/eli/Multilingual_IE_Seismic/emissions.csv\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from carbontracker.tracker import CarbonTracker\n",
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "#tracker1 = CarbonTracker(epochs=1, decimal_precision=8) # Carbontracker\n",
    "tracker = EmissionsTracker() # CodeCarbon\n",
    "\n",
    "tracker = EmissionsTracker()\n",
    "tracker.start()\n",
    "try:\n",
    "      # Compute intensive code goes here\n",
    "      index = SeismicIndex.build(json_input_file)\n",
    "finally:\n",
    "      tracker.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2b50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = SeismicIndex.build(\n",
    "    compressed_input_file,\n",
    "    n_postings=3500,\n",
    "    centroid_fraction=0.1,\n",
    "    min_cluster_size=2,\n",
    "    summary_energy=0.4, \n",
    "    batched_indexing=10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d4942e",
   "metadata": {},
   "source": [
    "### By setting the `nknn` parameter we can build the knn graph together with the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f6f899",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = SeismicIndex.build(\n",
    "    json_input_file,\n",
    "    n_postings=3500,\n",
    "    centroid_fraction=0.1,\n",
    "    min_cluster_size=2,\n",
    "    summary_energy=0.4,\n",
    "    nknn=10,\n",
    "    batched_indexing=10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45135b50",
   "metadata": {},
   "source": [
    "### While, if we set also the `knn_path` (details on how to do it below), we can add to the index a precomputed knn graph. In this case, the `nknn` parameter allow us to add a subset of the knn graph (with less neighbors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5305aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_path = \"\"\n",
    "\n",
    "index = SeismicIndex.build(\n",
    "    json_input_file,\n",
    "    n_postings=3500,\n",
    "    centroid_fraction=0.1,\n",
    "    min_cluster_size=2,\n",
    "    summary_energy=0.4,\n",
    "    knn_path=knn_path,\n",
    "    nknn=5,\n",
    "    batched_indexing=10000000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eecf33",
   "metadata": {},
   "source": [
    "### Once the index is constructed, we can serialize and store it in a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06de631d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m index_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmMARCO/BM25_results/English_IndexmMARCO/BM25_results/English_Index\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[43mindex\u001b[49m\u001b[38;5;241m.\u001b[39msave(index_path)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'index' is not defined"
     ]
    }
   ],
   "source": [
    "index_path = \"mMARCO/BM25_results/English_IndexmMARCO/BM25_results/English_Index\"\n",
    "\n",
    "index.save(index_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f572d70",
   "metadata": {},
   "source": [
    "## 1.2 Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8c20f59",
   "metadata": {},
   "source": [
    "### We may want to load a serialized index to query it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44532aed",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = \"\"\n",
    "\n",
    "index = SeismicIndex.load(index_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e1c150",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Number of documents: \", index.len)\n",
    "print(\"Avg number of non-zero components: \", index.nnz / index.len)\n",
    "print(\"Dimensionality of the vectors: \", index.dim)\n",
    "\n",
    "index.print_space_usage_byte()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f618f314",
   "metadata": {},
   "source": [
    "# 2. kNN Graph\n",
    "\n",
    "### Given an inverted index, we can build a knn graph and attach to it with the build_knn function. It is also possible to serialize the graph and link it to another index with the `load_knn` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946322aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "nknn=10\n",
    "index.build_knn(nknn)\n",
    "\n",
    "knn_path = \"\"\n",
    "\n",
    "index.save_knn(knn_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0034c7",
   "metadata": {},
   "source": [
    "### When adding the knn graph we can specify a subset of the neighbours we want for each entry of the index or load the full knn graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f93fb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_path = \"\"\n",
    "knn_path = \"\"\n",
    "\n",
    "#load full knn graph\n",
    "index.load_knn(knn_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f27085b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "nknn = 5\n",
    "\n",
    "#load partial graph\n",
    "index.load_knn(knn_path, nknn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73a4d8a",
   "metadata": {},
   "source": [
    "# 3. Perform the search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93ed4c1a",
   "metadata": {},
   "source": [
    "### Prepare the data to perform the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46e2f0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "file_path = \"\"\n",
    "\n",
    "queries = []\n",
    "with open(file_path, 'r') as f:\n",
    "    for line in f:\n",
    "        queries.append(json.loads(line))\n",
    "\n",
    "MAX_TOKEN_LEN = 30\n",
    "string_type  = f'U{MAX_TOKEN_LEN}'\n",
    "\n",
    "queries_ids = np.array([q['id'] for q in queries], dtype=string_type)\n",
    "\n",
    "query_components = []\n",
    "query_values = []\n",
    "\n",
    "for query in queries:\n",
    "    vector = query['vector']\n",
    "    query_components.append(np.array(list(vector.keys()), dtype=string_type))\n",
    "    query_values.append(np.array(list(vector.values()), dtype=np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8286c10",
   "metadata": {},
   "source": [
    "### We can ran a single search or a parallel batch search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f68dc58",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = index.search(\n",
    "    query_id=str(queries_ids[0]),\n",
    "    query_components=query_components[0],\n",
    "    query_values=query_values[0],\n",
    "    k=10,\n",
    "    query_cut=20,\n",
    "    heap_factor=0.7,\n",
    "    n_knn=0,\n",
    "    sorted=True, #specified even if default value\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5682de5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = index.batch_search(\n",
    "    queries_ids=queries_ids,\n",
    "    query_components=query_components,\n",
    "    query_values=query_values,\n",
    "    k=10,\n",
    "    query_cut=20,\n",
    "    heap_factor=0.7,\n",
    "    n_knn=0,\n",
    "    sorted=True, #specified even if default value\n",
    "    num_threads=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6b6cf4",
   "metadata": {},
   "source": [
    "# 4. Evaluation of results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4216faaa",
   "metadata": {},
   "source": [
    "### Evaluation of the results with the ir_measure library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90784514",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ir_measures\n",
    "import ir_datasets\n",
    "\n",
    "# add your ir_dataset dataset string id below, e.g., \"beir/quora/test\"\n",
    "ir_dataset_string = \"\"\n",
    "\n",
    "ir_results = [ir_measures.ScoredDoc(query_id, doc_id, score) for r in results for (query_id, score, doc_id) in r]\n",
    "qrels = ir_datasets.load(ir_dataset_string).qrels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0aa5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ir_measures import *\n",
    "\n",
    "measure_to_compute = \"RR@10\"\n",
    "ir_measures.calc_aggregate([measure_to_compute], qrels, ir_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a67154",
   "metadata": {},
   "source": [
    "# 5. Raw Seismic Index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7afb44c",
   "metadata": {},
   "source": [
    "### Raw Seismic Index: input a file in the Seismic internal format, i.e., as the plain Rust index. See how to use the script `scripts/convert_json_to_inner_format.py`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d2e63df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from seismic import SeismicIndexRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f21e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = \"\"\n",
    "\n",
    "index = SeismicIndexRaw.build(input_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3d2654",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_path = \"\"\n",
    "\n",
    "results = index.batch_search(\n",
    "    query_path,\n",
    "    k=10,\n",
    "    query_cut=3,\n",
    "    heap_factor=0.9,\n",
    "    n_knn=0,\n",
    "    sorted=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
